---
layout: page
title: Benchmarks
permalink: /benchmarks/
nav_order: 6
---

<h2> Benchmarking </h2>

Incomplete. Need to discuss with Ashok.

<!-- It is important for a study to have reproducible results so the algorithms can be tested on other datasets for validity. Not only do these algorithms include estimation models, but they also include the data filtering algorithms that may shift the training and testing set distributions. We benchmark the data filtering algorithms and model combinations on publically available datasets. The implementation was done based on the code provided (on GitHub) and/or the details of the algorithms presented in the paper. To see these references, see <a href="{{site.baseurl}}/code/">Code</a>.

{% include replicated_ed_scatter.html %}

<center> <i> Fig. 1. Reported Explained Deviation vs Replicated Explained Deviation. The different datasets are straitified by shape and color. Hovering over the scatter points provides more information about the benchmarked study. </i></center>
 -->
<h2> Open Contribution</h2>

In addition to the implemented benchmarks, we provide an opportunity to contribute to this table by providing an update form for researchers to improve upon existing benchmarks and add new benchmarks. Every benchmark reported on our website will follow strict guidelines on sharing code and reporting results. We provide examples and templates for feature extraction machine learning pipelines, deep learning pipelines, and data visualization scripts to allow for more transparent reporting and streamlined testing. This code can be found on in our WearableBP [GitHub](https://wearablebp.github.io). Finally, although the benchmarks only cover PPG, as wearable sensors become cheaper and more ubiquitous, they will be expanded to include them.

Forms to contribute to our benchmarks:
1. [Submit a Dataset](https://forms.gle/XeP3udcv72vfkrGM6)
2. [Submit an algorithm](https://forms.gle/YG19pWmvvradVVaS8)
